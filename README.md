# sparkify_data_lake
A data lake and ETL job built with Spark on AWS EMR

# Requirements
* a file named named dl.cfg with the contents: "AWS_ACCESS_KEY_ID='YOUR_KEY_ID_HERE' AWS_SECRET_ACCESS_KEY='YOUR_SECREY_KEY_HERE', with the values for your cluster installed.

